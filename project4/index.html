<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Part 1 – Understanding the Forward and Reverse Diffusion Processes</title>
<style>
    body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }
    h2 { margin-top: 40px; }
    figure { text-align: center; margin: 20px auto; }
    figcaption { font-style: italic; margin-top: 8px; }
    img { max-width: 90%; border: 1px solid #ccc; padding: 4px; border-radius: 4px; }
</style>
</head>

<body>

<h1>Part 1: Understanding the Forward and Reverse Diffusion Processes</h1>

<p>
In this part of the project, my goal was to understand how diffusion models add noise to an image 
(forward process) and how they recover the original structure (reverse denoising). I experimented 
with different noise levels, observed how the UNet predicts noise, and compared different denoising 
strategies such as one–step denoising, iterative denoising, and Gaussian blur. Below, I explain 
each subsection clearly in my own words along with the observations.
</p>

<!-- ----------------------------------------------------------- -->
<h2>1.1 Forward Process (Adding Noise)</h2>

<p>
In this section, I applied the forward diffusion equation to the original image using three 
different timesteps (<b>t = 250, 500, 750</b>). As <i>t</i> increases, the amount of noise increases. 
The purpose of this experiment is to understand how diffusion models gradually corrupt an image.
</p>

<p><b>Observations:</b></p>
<ul>
    <li>At lower <i>t</i> (250), the coarse structure of the tower is still visible.</li>
    <li>At medium noise (500), most details disappear and only rough shapes remain.</li>
    <li>At high noise (750), the image becomes almost pure Gaussian noise.</li>
</ul>

<figure>
    <img src="1.1_one.png" alt="1.1 noisy images">
    <figcaption>Noisy images at different timesteps (t = 250, 500, 750).</figcaption>
</figure>

<!-- ----------------------------------------------------------- -->
<h2>1.2 Gaussian Blur Baseline</h2>

<p>
Before using the diffusion model for denoising, I first used a simple Gaussian blur to see how 
well traditional filtering performs. This part helps to compare classical denoising with learned 
denoising.
</p>

<p><b>Observations:</b></p>
<ul>
    <li>Gaussian blur removes noise but also destroys the fine edges.</li>
    <li>The result looks like a smeared version of the original.</li>
    <li>Compared to diffusion denoising, this method cannot recover natural image structure.</li>
</ul>

<figure>
    <img src="1.2_one.png" alt="1.2 gaussian blur img1"><br><br>
    <img src="1.2_two.png" alt="1.2 gaussian blur img2">
    <figcaption>Gaussian blur applied to noisy images (t = 250 and t = 500).</figcaption>
</figure>

<!-- ----------------------------------------------------------- -->
<h2>1.3 One–Step Denoising Using the UNet</h2>

<p>
Here, I used the diffusion model's UNet to predict the noise in a single step and subtract it from 
the noisy image. This is effectively the model’s estimate of <i>x<sub>0</sub></i> using Equation (2) 
from the DDPM paper.
</p>

<p><b>Observations:</b></p>
<ul>
    <li>The model reconstructs sharper edges compared to Gaussian blur.</li>
    <li>As noise increases, the UNet’s one–step prediction becomes less accurate.</li>
    <li>It still fails to fully reconstruct the image at high noise (e.g., t = 750).</li>
</ul>

<figure>
    <img src="1.3_one.png" alt="1.3 one-step img1"><br><br>
    <img src="1.3_two.png" alt="1.3 one-step img2"><br><br>
    <img src="1.3_three.png" alt="1.3 one-step img3">
    <figcaption>One-step denoising results for t = 250, t = 500, and t = 750.</figcaption>
</figure>

<!-- ----------------------------------------------------------- -->
<h2>1.4 Iterative Denoising (DDPM Reverse Process)</h2>

<p>
In this section, I implemented the full iterative DDPM denoising process using a strided schedule 
of timesteps (<code>[990, 960, ..., 0]</code>). Instead of predicting the clean image in one step, the 
model gradually reduces the noise over many steps.
</p>

<p><b>Observations:</b></p>
<ul>
    <li>The image gets progressively cleaner as we move through the timestep list.</li>
    <li>The iterative result is much sharper and smoother than the one–step reconstruction.</li>
    <li>It successfully reconstructs the tower structure even from very noisy inputs.</li>
</ul>

<figure>
    <img src="1.4_one.png" alt="1.4 progressive denoising">
    <figcaption>Progressive denoising: images shown every 5 steps from i = 10 to i = 32.</figcaption>
</figure>

<figure>
    <img src="1.4_two.png" alt="1.4 comparison">
    <figcaption>Final comparison of iterative denoising, one–step denoising, and Gaussian blur.</figcaption>
</figure>

<p>
Overall, Part 1 clearly demonstrates how diffusion models remove noise more effectively than 
classical filtering approaches, and how iterative denoising significantly improves reconstruction 
quality compared to one-step methods.
</p>
<hr><br>

<h1>Part 2: Understanding the Forward and Reverse Processes</h1>

<p>
In Part 2, we moved beyond simple denoising and explored how diffusion models can be used for 
<i>generation, guidance-based sampling, and image-to-image translation</i>.  
All experiments used the same pretrained DeepFloyd IF Stage 1 UNet, and the same scheduler and prompt 
embeddings that were prepared earlier in the notebook.
</p>

<!-- --------------------------------------------------------------- -->
<h2>2.1 Diffusion Model Sampling (Image Generation from Noise)</h2>

<p>
The goal of this section was to generate images <b>from pure Gaussian noise</b>, using the
<code>iterative_denoise</code> function developed in Part 1.4.  
By setting <code>i_start = 0</code>, we begin the reverse diffusion chain from the noisiest timestep and 
let the model gradually push the noise back onto the natural image manifold.  
The prompt used for all samples was <code>"a high quality photo"</code>.
</p>

<p>
The important observation here is that the model is forced to hallucinate an image entirely from 
its learned distribution; therefore, the results tend to be blurry or low-detail when no guidance 
mechanism is used. This is expected because the reverse process is running without any additional 
constraints and relies purely on the denoiser's learned prior.
</p>

<figure>
    <img src="2.1_one.png" alt="2.1 generative noise samples" style="max-width:85%;">
    <figcaption>Five images generated from pure noise using iterative denoising.</figcaption>
</figure>

<!-- --------------------------------------------------------------- -->
<h2>2.2 Classifier-Free Guidance (CFG)</h2>

<p>
To improve the quality of the generated images, we implemented  
<b>Classifier-Free Guidance (CFG)</b>.  
The key idea is to combine two noise predictions from the UNet:
</p>

<ul>
    <li>a <em>conditional</em> noise estimate (ε<sub>c</sub>) using the text prompt,</li>
    <li>an <em>unconditional</em> noise estimate (ε<sub>u</sub>) using an empty prompt,</li>
</ul>

<p>
and interpolate them using the CFG scale γ:
</p>

<p style="text-align:center; font-size:18px;">
ε = ε<sub>u</sub> + γ (ε<sub>c</sub> − ε<sub>u</sub>)
</p>

<p>
We used <b>γ = 7</b>, which typically strengthens the conditioning signal.<br>
Compared to Part 2.1, the images produced here were more structured, had smoother color transitions, 
and looked less like random noise.  
Although the model still produced abstract results due to limited training resolution,
there was a clear improvement over unguided sampling.
</p>

<figure>
    <img src="2.2_one.png" alt="CFG results" style="max-width:85%;">
    <figcaption>Five CFG-guided samples using γ = 7. Noticeably sharper and more structured than unguided samples.</figcaption>
</figure>

<!-- --------------------------------------------------------------- -->
<h2>2.3 Image-to-Image Translation (SDEdit)</h2>

<p>
In this section, we applied the diffusion model to perform <b>image-to-image translation</b>, 
inspired by the SDEdit algorithm.  
The idea is:
</p>

<ul>
    <li>Start from a real image x₀.</li>
    <li>Add forward-process noise at a chosen timestep t.</li>
    <li>Use reverse diffusion (with CFG) to project the noisy image back to the natural image manifold.</li>
</ul>

<p>
By changing the starting timestep t (equivalently the <code>i_start</code> index),  
we control how much “creativity” or modification the model introduces:
</p>

<ul>
    <li><b>Small noise</b> → output remains close to the original.</li>
    <li><b>Moderate noise</b> → model begins altering colors/shapes.</li>
    <li><b>Large noise</b> → model produces entirely new interpretations while keeping global structure.</li>
</ul>

<p>
We performed this experiment on:
</p>

<ol>
    <li>the provided test tower image,</li>
    <li>a flower image (own image),</li>
    <li>a cat image (own image).</li>
</ol>

<p>
Across all three cases, we observed a consistent pattern: increasing <code>i_start</code> causes the model to 
inject more “creative freedom,” gradually morphing the original picture while still keeping 
the overall scene composition (e.g., background layout, shapes, or silhouettes).
</p>

<!-- Flower results -->
<figure>
    <img src="2.3_one.png" alt="Flower SDEdit" style="max-width:75%;"><br><br>
    <img src="2.3_two.png" alt="Flower SDEdit more" style="max-width:75%;">
    <figcaption>SDEdit results on the flower image. Larger i_start values produce more creative, stylized outputs.</figcaption>
</figure>

<!-- Cat results -->
<figure>
    <img src="2.3_three.png" alt="Cat SDEdit" style="max-width:75%;"><br><br>
    <img src="2.3_four.png" alt="Cat SDEdit more" style="max-width:75%;">
    <figcaption>SDEdit results on the cat image. Noise controls the level of abstraction and reconstruction.</figcaption>
</figure>

<!-- Tower results -->
<figure>
    <img src="2.3_five.png" alt="Tower SDEdit" style="max-width:75%;"><br><br>
    <img src="2.3_six.png" alt="Tower SDEdit more" style="max-width:75%;">
    <figcaption>SDEdit results on the tower image. Higher noise causes stronger reinterpretation while preserving scene layout.</figcaption>
</figure>

<p>
Overall, SDEdit demonstrates the flexibility of diffusion models in transforming images while 
preserving global structure. Even with low-resolution UNet models, the transition from light edits 
to heavy reinterpretation is clearly visible as the noise level increases.
</p>

<br><hr><br>

<hr><br>

<h1>Part 3: Visual Anagrams</h1>

<p>
In this part of the project, I implemented <b>Visual Anagrams</b> using the DeepFloyd IF diffusion model.
The goal of this section is to create a single diffusion image that shows one concept when it is viewed normally,
and a completely different concept when the image is flipped upside down.
This was one of the most interesting parts of the assignment because it requires controlling the generative model
during sampling, not after.
</p>

<h2>Approach</h2>

<p>For each image, I used two different text prompts:</p>

<ul>
    <li>one prompt for the <b>upright view</b></li>
    <li>one prompt for the <b>flipped view</b></li>
</ul>

<p>
During generation, I produced two noise predictions at every denoising step:
one from the upright prompt and one from the flipped prompt.
The flipped prediction was created by rotating the latent representation by 180 degrees.
By blending both noise predictions, the final image slowly forms a hidden structure that changes when the orientation is reversed.
</p>

<p>
Since the base model only generates 64×64 images, the results appear more abstract and blurry, which is expected.
To improve clarity, I also used the Stage-2 upsampling model to convert selected images to 256×256 resolution.
</p>

<p>The assignment requires:</p>

<ul>
    <li>a correct implementation of the visual anagram sampling procedure,</li>
    <li>one illusion of “campfire upright → old man when flipped,”</li>
    <li>two more illusions of our choice,</li>
    <li>example images showing upright vs. flipped behavior.</li>
</ul>

<p>
I generated three illusions total:
</p>

<ol>
    <li>Campfire → Old Man</li>
    <li>Waterfalls → Skull</li>
    <li>Dog → Man</li>
</ol>

<p>
For each illusion, I generated two 64×64 outputs using different seeds (six total).
Then I upsampled one output from each category to 256×256.
</p>


<h2>64×64 Visual Anagrams</h2>

<figure>
    <img src="fig_1.png" style="max-width:75%;">
    <figcaption>64×64 Visual Anagram — Campfire (upright) vs. Old Man (flipped).</figcaption>
</figure>

<figure>
    <img src="fig_2.png" style="max-width:75%;">
    <figcaption>Second seed — Campfire (upright) vs. Old Man (flipped).</figcaption>
</figure>

<figure>
    <img src="fig_3.png" style="max-width:75%;">
    <figcaption>64×64 Visual Anagram — Waterfalls (upright) vs. Skull (flipped).</figcaption>
</figure>

<figure>
    <img src="fig_4.png" style="max-width:75%;">
    <figcaption>Second seed — Waterfalls (upright) vs. Skull (flipped).</figcaption>
</figure>

<figure>
    <img src="fig_5.png" style="max-width:75%;">
    <figcaption>Second seed — Dog (upright) vs. Man (flipped).</figcaption>
</figure>


<h2>Observations</h2>

<p>Across all results, I observed that:</p>

<ul>
    <li>The upright and flipped views genuinely produce different shapes and textures.</li>
    <li>Even though the images are abstract, the overall silhouette changes noticeably.</li>
    <li>Waterfall/skull and dog/man illusions were especially clear because the geometry differed strongly.</li>
    <li>The campfire/old man illusion produced softer shapes, but facial-like edges appeared when flipped.</li>
    <li>The 256×256 upsampled images look cleaner and smoother, although still abstract due to the 64×64 base model.</li>
</ul>

<p>
Overall, visual anagrams work surprisingly well even with a small model, and flipping the image
produces a clear semantic shift in every case.
</p>


<h2>256×256 Upsampled Visual Anagrams (Three Images)</h2>

<figure>
    <img src="fig_6.png" style="max-width:75%;">
    <figcaption>256×256 Upsampled Visual Anagram — Campfire vs. Old Man.</figcaption>
</figure>

<figure>
    <img src="fig_7.png" style="max-width:75%;">
    <figcaption>256×256 Upsampled Visual Anagram — Waterfalls vs. Skull.</figcaption>
</figure>

<figure>
    <img src="fig_8.png" style="max-width:75%;">
    <figcaption>256×256 Upsampled Visual Anagram — Dog vs. Man.</figcaption>
</figure>


<h2>Conclusion</h2>

<p>
This part of the project successfully demonstrates how diffusion models can be manipulated to embed
two different interpretations into a single generated image.
Even though the model generates low-resolution outputs, the upright and flipped views consistently
show different semantic structures.
This experiment highlights the flexibility of diffusion models and shows how prompt guidance and
geometric transformations can be combined to produce creative and surprising generative effects.
</p>

<br><hr><br>


</body>
</html>

